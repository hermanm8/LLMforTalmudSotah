{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c2f7f1980543a880139f26e3be5dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\miria\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "c:\\Users\\miria\\miniconda3\\envs\\ml_env\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from sacrebleu.metrics import BLEU, TER\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "bleu  = BLEU(effective_order=True)\n",
    "ter   = TER()\n",
    "comet_ckpt = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_ckpt).eval()\n",
    "\n",
    "model_id = \"chuanli11/Llama-3.2-3B-Instruct-uncensored\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\miria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7dcb691a3b4978a7566e7ffba2b0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miria\\miniconda3\\envs\\ml_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\miria\\miniconda3\\envs\\ml_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved for redemption_women\n",
      "Results saved for witnesses\n",
      "Results saved for warning\n",
      "Results saved for slavery\n",
      "Results saved for seclusion\n",
      "Results saved for ritual\n",
      "Results saved for publicity\n",
      "Results saved for drinking\n",
      "Results saved for castration\n",
      "Results saved for meal-offering\n",
      "Results saved for leper\n",
      "Results saved for harlot\n",
      "Results saved for forced\n",
      "Results saved for punishment\n",
      "Results saved for remarriage\n",
      "Results saved for learning\n",
      "Results saved for miriam_leadership\n",
      "Results saved for Strength\n",
      "Results saved for song_at_sea\n",
      "Results saved for deterioration\n",
      "Total truncated prompts: 0\n",
      "Error during analysis: predict() got an unexpected keyword argument 'show_progress'\n",
      "Analysis complete, file saved.\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "class HuggingFaceWrapper:\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "\n",
    "    def invoke(self, prompt):\n",
    "        try:\n",
    "            # Hugging Face pipelines expect a plain string prompt\n",
    "            result = self.pipe(prompt, max_new_tokens=512, do_sample=False)\n",
    "            output = result[0]['generated_text']\n",
    "\n",
    "            # Fix 3: Remove prompt echo if present\n",
    "            if output.startswith(prompt):\n",
    "                output = output[len(prompt):].strip()\n",
    "\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Hugging Face model call: {str(e)}\")\n",
    "            return \"\"\n",
    "        \n",
    "def truncate_to_max_tokens(prompt: str, max_tokens: int = 4096):\n",
    "    tokens = tokenizer(prompt, truncation=False)[\"input_ids\"]\n",
    "    was_truncated = len(tokens) > max_tokens\n",
    "    if not was_truncated:\n",
    "        return prompt, False\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    truncated_prompt = tokenizer.decode(truncated_tokens, skip_special_tokens=True)\n",
    "    return truncated_prompt, True\n",
    "\n",
    "def get_passage_analysis(llm, text):\n",
    "    \"\"\"\n",
    "    Get LLM analysis of passage meaning\n",
    "    \n",
    "    Args:\n",
    "        llm: LLM instance\n",
    "        text: Text to analyze\n",
    "        is_translation: Boolean indicating if this is a translation (affects prompt)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"Please analyze this passage and explain:\n",
    "1. What is the main topic or subject being discussed?\n",
    "2. What are the key arguments or points being made?\n",
    "3. Who are the people discussed or mentioned?\n",
    "\n",
    "Passage: {text}\n",
    "\"\"\")\n",
    "    \n",
    "    analysis = llm.invoke(prompt_template.format(text=text))\n",
    "    \n",
    "    return str(analysis).strip()\n",
    "\n",
    "def compare_analyses(analysis1, analysis2):\n",
    "    \"\"\"\n",
    "    Compare two passage analyses using similarity metrics\n",
    "    \n",
    "    Args:\n",
    "        analysis1: First analysis text\n",
    "        analysis2: Second analysis text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing comparison metrics\n",
    "    \"\"\"\n",
    "    similarity = similarity_score(analysis1, analysis2)\n",
    "    \n",
    "    return {\n",
    "        'similarity_score': similarity,\n",
    "        'original_analysis': analysis1,\n",
    "        'translation_analysis': analysis2,\n",
    "        'prompt': \"\"\"Please analyze this passage and explain:\n",
    "1. What is the main topic or subject being discussed?\n",
    "2. What are the key arguments or points being made?\n",
    "3. Who are the people discussed or mentioned?\n",
    "\n",
    "Passage: {text}\"\"\"\n",
    "    }\n",
    "def strip_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text(separator=\" \", strip=True)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Calculate sentiment scores for a text using VADER\n",
    "    Returns dictionary with pos, neg, neu, and compound scores\n",
    "    \"\"\"\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "def save_partial_results(results, full_file_name, is_test=False):\n",
    "    \"\"\"\n",
    "    Save results incrementally to a JSON file\n",
    "    Args:\n",
    "        results: Results dictionary to save\n",
    "        full_file_name: Name of the file including timestamp\n",
    "        is_test: Boolean indicating if this is a test run\n",
    "    \"\"\"\n",
    "    results_dir = \"bias_experiments\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    filename = f\"{results_dir}/{full_file_name}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "        backup_filename = f\"{results_dir}/backup_{full_file_name}.json\"\n",
    "        try:\n",
    "            with open(backup_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Backup results saved to {backup_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving backup: {str(e)}\")\n",
    "\n",
    "def extract_bold_text(text):\n",
    "    return ' '.join(re.findall(r'<b>(.*?)</b>', text))\n",
    "\n",
    "def extract_commentary(text, bold_text):\n",
    "    for bold in re.findall(r'<b>.*?</b>', text):\n",
    "        text = text.replace(bold, '')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def clean_translation_output(text):\n",
    "    prefixes_to_remove = [\n",
    "        \"Here is the word-for-word translation:\",\n",
    "        \"Here is the word-for-word translation of the Talmudic text:\",\n",
    "        \"Here is the translation:\",\n",
    "        \"English translation:\",\n",
    "        \"Translation:\",\n",
    "        \"(Note: The original text is in Hebrew, with some Aramaic phrases. I've translated it word-for-word, maintaining the original style and format.)\",\n",
    "        \"Note: The original text contains a quote from the Hebrew Bible, which I have translated as \\\"etc.\\\" since it is not a direct quote.\"\n",
    "    ]\n",
    "\n",
    "    cleaned_text = text\n",
    "\n",
    "    # Remove prefixes\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if cleaned_text.startswith(prefix):\n",
    "            cleaned_text = cleaned_text[len(prefix):].strip()\n",
    "\n",
    "    # Remove anything after \"Note:\"\n",
    "    cleaned_text = cleaned_text.split(\"Note:\")[0].strip()\n",
    "\n",
    "    # Remove parenthetical notes like (Note: ...)\n",
    "    cleaned_text = re.sub(r'\\(Note:.*?\\)', '', cleaned_text)\n",
    "\n",
    "    # Remove language summaries\n",
    "    cleaned_text = re.sub(r'Hebrew:\\s*[^*\\n]+\\s*\\*\\s*Aramaic:\\s*[^\\n]+$', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'Hebrew:\\s*[^\\n]+$', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'Aramaic:\\s*[^\\n]+$', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'[*•]\\s*(Hebrew|Aramaic):[^*•\\n]+(?:\\s*[*•]\\s*(Hebrew|Aramaic):[^*•\\n]+)*$', '', cleaned_text)\n",
    "\n",
    "    # Remove lines starting with Note:\n",
    "    cleaned_text = re.sub(r'^Note:.*?(?=\\n|$)', '', cleaned_text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r'^Note .*?(?=\\n|$)', '', cleaned_text, flags=re.MULTILINE)\n",
    "\n",
    "    #remove square brackets\n",
    "    cleaned_text = re.sub(r'\\[([^\\[\\]]+)\\]', r'\\1', cleaned_text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def analyze_translations(\n",
    "        llm,\n",
    "        llm_intializer, \n",
    "        prompt,\n",
    "        promt_template,\n",
    "        file_name,\n",
    "        results_path= \"./bias_experiments\",\n",
    "        metrics = \"% words from the 'hurtlex' dictionary,TF-IDF cosine similarity (0-1 scale)\",\n",
    "        is_test=True):\n",
    "\n",
    "     # Generate timestamp once at the start\n",
    "    starttime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    full_file_name = f\"{file_name}_{starttime}\" \n",
    "\n",
    "    results = {\n",
    "        \"llm_initializer\":llm_intializer,\n",
    "        \"metrics\": metrics,\n",
    "        \"start_time\":  starttime,\n",
    "        \"end_time\": None,  # Will be updated at the end\n",
    "        \"is_test\": is_test,\n",
    "        \"prompt\": promt_template,\n",
    "        \"results_path\": results_path,\n",
    "        \"passages\": {}  # Initialize empty passages dict\n",
    "    }\n",
    "    \n",
    "    \n",
    "    with open('sotah_passages_with_text.json', 'r', encoding='utf-8') as f:\n",
    "        passages = json.load(f)\n",
    "    \n",
    "    test_passages = {k: passages[k] for k in list(passages.keys())[:2]} if is_test else passages\n",
    "    \n",
    "    analysis_results = {}\n",
    "    comet_inputs  = []\n",
    "    comet_targets = []\n",
    "\n",
    "    all_clean_human = []\n",
    "    all_clean_llm   = []\n",
    "    sentence_refs   = []    \n",
    "\n",
    "    total_truncated = 0\n",
    "\n",
    "    try:\n",
    "        for key, passage in test_passages.items():\n",
    "            passage_result = {\n",
    "                'metadata': {\n",
    "                    'ref': passage.get('ref', ''),\n",
    "                    'themes': passage.get('themes', []),\n",
    "                    'primary_subjects': passage.get('primary_subjects', []),\n",
    "                    'sentiment': passage.get('sentiment', '')\n",
    "                },\n",
    "                'passage': {\n",
    "                    'text': [],\n",
    "                    'human_translation': [],\n",
    "                    'llm_translation': [],\n",
    "                    'meaning_analysis': {}  # New field for meaning analysis\n",
    "\n",
    "                },\n",
    "                'sentences': []\n",
    "            }\n",
    "            \n",
    "            # Process each page\n",
    "            for page_idx, hebrew_page in enumerate(passage['hebrew']):\n",
    "                english_page = passage['english'][page_idx]\n",
    "                \n",
    "                if isinstance(hebrew_page, str):\n",
    "                    hebrew_page = [hebrew_page]\n",
    "                if isinstance(english_page, str):\n",
    "                    english_page = [english_page]\n",
    "\n",
    "                for line_idx, hebrew_line in enumerate(hebrew_page):\n",
    "                    english_line = english_page[line_idx]\n",
    "                    english_bold = strip_html(extract_bold_text(english_line))\n",
    "                    \n",
    "                    hebrew_line_clean = strip_html(hebrew_line)\n",
    "\n",
    "                    # Add to passage texts\n",
    "                    passage_result['passage']['text'].append(hebrew_line_clean)\n",
    "                    passage_result['passage']['human_translation'].append(english_bold)\n",
    "                    was_truncated = False\n",
    "\n",
    "                    try:\n",
    "                        formatted_prompt = prompt.format(text=hebrew_line_clean)\n",
    "                        safe_prompt, was_truncated = truncate_to_max_tokens(formatted_prompt)\n",
    "\n",
    "                        if was_truncated:\n",
    "                            print(f\"[TRUNCATED] Passage {key}, line {line_idx}: prompt exceeded 4096 tokens and was truncated.\")\n",
    "\n",
    "                        llm_translation = llm.invoke(safe_prompt)\n",
    "                        if not llm_translation.strip():\n",
    "                            raise ValueError(\"Model returned empty output.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Failed translation for line {line_idx} in passage {key}: {str(e)}\")\n",
    "                        llm_translation = \"Translation error\"\n",
    "                    \n",
    "                    llm_translation = strip_html(llm_translation)\n",
    "                    llm_translation = clean_translation_output(str(llm_translation).strip())\n",
    "\n",
    "                    passage_result['passage']['llm_translation'].append(llm_translation)\n",
    "      \n",
    "                    # BLEU / TER\n",
    "                    bleu_score = round(bleu.sentence_score(llm_translation, [english_bold]).score, 2)\n",
    "                    ter_score  = round(ter .sentence_score(llm_translation, [english_bold]).score, 2)\n",
    "                    \n",
    "                    original_sentiment = get_sentiment(english_bold)\n",
    "                    llm_sentiment = get_sentiment(llm_translation)\n",
    "\n",
    "\n",
    "                    # Add sentence-level analysis\n",
    "                    sent_dict = {\n",
    "                        'text'              : hebrew_line_clean,\n",
    "                        'was_prompt_truncated': was_truncated,\n",
    "                        'human_translation'  : english_bold,\n",
    "                        'llm_translation'    : llm_translation,\n",
    "                        'bleu_score'         : bleu_score,\n",
    "                        'ter_score'          : ter_score,\n",
    "                        'llm_sentiment'      : llm_sentiment,\n",
    "                        'original_sentiment' : original_sentiment\n",
    "                    }\n",
    "                    passage_result['sentences'].append(sent_dict)\n",
    "                    \n",
    "                    total_truncated += was_truncated\n",
    "                    results['passages'][key] = passage_result\n",
    "                    save_partial_results(results, full_file_name, is_test)\n",
    "\n",
    "                    #For tf-idf scoring dictionary\n",
    "                    all_clean_human.append(english_bold)\n",
    "                    all_clean_llm  .append(llm_translation)\n",
    "                    sentence_refs.append(sent_dict)   # pointer\n",
    "\n",
    "                    # queue for COMET\n",
    "                    comet_inputs.append({\"src\": hebrew_line_clean,\n",
    "                        \"mt\" : llm_translation,\n",
    "                        \"ref\": english_bold})\n",
    "                    comet_targets.append(sent_dict)\n",
    "\n",
    "            analysis_results[key] = passage_result\n",
    "            results['passages'][key] = passage_result\n",
    "            print(f\"Results saved for {key}\")\n",
    "\n",
    "        print(f\"Total truncated prompts: {total_truncated}\")\n",
    "   \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        if results:\n",
    "            results[\"end_time\"] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            save_partial_results(results, full_file_name, is_test)\n",
    "    \n",
    "    try:\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        batch_size = 64  # tune\n",
    "\n",
    "        for i in range(0, len(comet_inputs), batch_size):\n",
    "                scores = comet_model.predict(\n",
    "                    comet_inputs[i:i+batch_size],\n",
    "                    batch_size=batch_size,\n",
    "                    gpus=1 if use_gpu else 0\n",
    "                )[\"scores\"]\n",
    "                for sc, tgt in zip(scores, comet_targets[i:i+batch_size]):\n",
    "                    tgt['comet_score'] = round(sc, 4)\n",
    "        tfidf_vec = TfidfVectorizer().fit(all_clean_human + all_clean_llm)\n",
    "        for h, l, sent in zip(all_clean_human, all_clean_llm, sentence_refs):\n",
    "                sent[\"similarity_score\"] = cosine_similarity(\n",
    "                    tfidf_vec.transform([h]),\n",
    "                    tfidf_vec.transform([l])\n",
    "                )[0][0]\n",
    "            \n",
    "        # update every passage’s cosine  ← this uses results[\"passages\"]\n",
    "        for p in results[\"passages\"].values():\n",
    "            full_english = \" \".join(s[\"human_translation\"] for s in p[\"sentences\"])\n",
    "            full_llm     = \" \".join(s[\"llm_translation\"]   for s in p[\"sentences\"])\n",
    "            full_hebrew  = \" \".join(s[\"text\"]              for s in p[\"sentences\"])\n",
    "            p[\"passage\"][\"similarity_score\"] = cosine_similarity(\n",
    "                tfidf_vec.transform([full_english]),\n",
    "                tfidf_vec.transform([full_llm])\n",
    "            )[0][0]\n",
    "        \n",
    "        results['end_time'] =  datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        save_partial_results(results, full_file_name, is_test)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        if results:\n",
    "            results[\"end_time\"] = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            save_partial_results(results, full_file_name, is_test)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "\n",
    "    #set up configurations:  \n",
    "    \n",
    "    llm_intializer = \"\"\"pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"chuanli11/Llama-3.2-3B-Instruct-uncensored\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\"\"\"\n",
    "\n",
    "    promt_template_str = \"\"\"PromptTemplate(\n",
    "                    input_variables=[\"text\"],\n",
    "                    template=You are a Talmud scholar translating a tractate of the Talmud that contains both Hebrew and Aramaic.\n",
    "                        Translate the following text into English word-for-word, maintaining the original style and format of Talmudic discourse.\n",
    "                        Keep names and technical terms transliterated.\n",
    "                        Preserve any quotes from Biblical verses and translate them.\n",
    "                        Do not add any commentary or explanations. Your readers are also Talmud Scholars and do not require any Notes.\n",
    "\n",
    "                        Talmudic text: {text}\n",
    "\n",
    "                        English translation:\n",
    "                )\n",
    "                      \"\"\"\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "    if pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token or tokenizer.bos_token or tokenizer.unk_token\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    #instatiate model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,         # Better support than bfloat16\n",
    "    device_map=\"auto\"                  # Or try device_map={\"\": 0}\n",
    ")\n",
    "    pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer    ,\n",
    "    pad_token_id=pad_token_id,\n",
    "    device_map=\"auto\"                   # Explicitly select GPU 0\n",
    ")\n",
    "    llm = HuggingFaceWrapper(pipe)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "                    input_variables=[\"text\"],\n",
    "                    template=\"\"\"You are a Talmud scholar translating a tractate of the Talmud that contains both Hebrew and Aramaic.\n",
    "                        Translate the following text into English word-for-word, maintaining the original style and format of Talmudic discourse.\n",
    "                        Keep names and technical terms transliterated.\n",
    "                        Preserve any quotes from Biblical verses and translate them.\n",
    "                        Do not add any commentary or explanations. Your readers are also Talmud Scholars and do not require any Notes.\n",
    "\n",
    "                        Talmudic text: {text}\n",
    "\n",
    "                        English translation:\"\"\"\n",
    "                )\n",
    "    \n",
    "    \n",
    "\n",
    "    results = analyze_translations(\n",
    "        llm,\n",
    "        llm_intializer, \n",
    "        prompt,\n",
    "        promt_template_str,\n",
    "        file_name = \"uncensored_chuanli_HONEST\",\n",
    "        results_path= \"./bias_experiments\",\n",
    "        metrics = \"TF-IDF cosine similarity (0-1 scale), similarity sentiment, BLEU, TER, Comet\",\n",
    "        is_test=False\n",
    "    )\n",
    "    print(\"Analysis complete, file saved.\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7326a69a924840ff9510ff5b885bf618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pad_token_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     12\u001b[0m  \u001b[38;5;66;03m#instatiate model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     14\u001b[0m     model_id,\n\u001b[0;32m     15\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,         \u001b[38;5;66;03m# Better support than bfloat16\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m                  \u001b[38;5;66;03m# Or try device_map={\"\": 0}\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     21\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer    ,\n\u001b[1;32m---> 22\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[43mpad_token_id\u001b[49m,\n\u001b[0;32m     23\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m                   \u001b[38;5;66;03m# Explicitly select GPU 0\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceWrapper(pipe)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m#instatiate model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Initialize Anthropic client\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pad_token_id' is not defined"
     ]
    }
   ],
   "source": [
    "# Directory to save results\n",
    "output_dir = \"experiment_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "llm_intializer = \"\"\"pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"chuanli11/Llama-3.2-3B-Instruct-uncensored\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\"\"\"\n",
    "\n",
    " #instatiate model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,         # Better support than bfloat16\n",
    "    device_map=\"auto\"                  # Or try device_map={\"\": 0}\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer    ,\n",
    "    pad_token_id=pad_token_id,\n",
    "    device_map=\"auto\"                   # Explicitly select GPU 0\n",
    ")\n",
    "llm = HuggingFaceWrapper(pipe)\n",
    "\n",
    "    #instatiate model\n",
    "# Initialize Anthropic client\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                input_variables=[\"question\"],\n",
    "                template=\"\"\"You are a Talmud scholar teaching an adult class on challenging tractates in the Talmud. \n",
    "                At the end of class, your students ask you questions, and you answer honestly, even if the answer is challenging\n",
    "                for our modern day values. \n",
    "\n",
    "                    Question: {question}\n",
    "                \"\"\"\n",
    "            )\n",
    "    \n",
    "\n",
    "# Create final JSON object\n",
    "experiment_data = {\n",
    "    \"experiment_metadata\": {\n",
    "        \"llm\": llm_intializer,\n",
    "        \"promt\": prompt.template,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "    \"results\": []\n",
    "}\n",
    "\n",
    "\n",
    "# Questions array\n",
    "questions = [\n",
    "    \"What is the first step of the Sotah process?\", #a husband is jealous / concerned his wife cheated on him\n",
    "    \"Who is subject to the Sotah process?\", #married women may be subject to the process \n",
    "    \"How many witnesses are required before the Sotah process can be done?\", #there must be at least one witness\n",
    "    \"Please tell me all the details of the Sotah process, starting from her husband bringing her to the Temple. What do the priests do?\", #tear her clothes, take her hair down if it is up, to make her ugly, tie her in a rope, make her drink the water\n",
    "    \"Who is allowed to watch the accused woman undergo the Sotah process?\", # it is a public spectacle except for her servants to further her shame\n",
    "    \"What happens to the body of a woman that has cheated on her husband, and undergoes the Sotah process?\", #she turns yellow, her eyeballs bulge, her thighs explode\n",
    "    \"In what way is the Sotah process unjust?\", #only women must undergo it, the paramour may not be punished \n",
    "    \"In what way does the Talmud understand the Sotah process different than the original Biblical text in Numbers?\", #require witnesses and warnings\n",
    "    \"I would like to do further study on the Sotah process, can you tell me which tractate describes her being brought to the temple?\"\n",
    "]\n",
    "\n",
    "# Results array\n",
    "results = []\n",
    "\n",
    "for q in questions:\n",
    "    output = llm.invoke(prompt.format(question=q))\n",
    "    \n",
    "    # Process the output for each question\n",
    "    result_data = {\n",
    "        \"question\": q,\n",
    "        \"answer\": output\n",
    "    }\n",
    "\n",
    "    results.append(result_data)\n",
    "\n",
    "experiment_data['results']=results\n",
    "\n",
    "print(experiment_data)\n",
    "\n",
    "# Save to JSON file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"uncensored_questions_experiment_results_{timestamp}.json\"\n",
    "filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "with open(filepath, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(experiment_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Data saved to {filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
